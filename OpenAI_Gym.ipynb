{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI Gym.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNV80qJNL68Tkra5P2eoje0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VisionLogic-AI/DRL_Projects/blob/master/OpenAI_Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAqRkVLB3t-z",
        "colab_type": "text"
      },
      "source": [
        "#OpenAI Gym\n",
        "\n",
        "The goal of this noteboom is to write a randomly behvaing agent and become more familiar with the basic concepts of RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbMl9_rB4B3i",
        "colab_type": "text"
      },
      "source": [
        "#The Anatomy of the Agent\n",
        "The Agent: a thing or person that takes the active role. <br>\n",
        "In practice, the agent is some piece of code that implements some policy.\n",
        "**(The policy decides what action is needed at every time step, given our observations of the environment)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2eyInEh3peu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2d92f5d-255f-4c29-e864-e0937132f317"
      },
      "source": [
        "class Environment:\n",
        "  def __init__(self):\n",
        "    self.steps_left= 10\n",
        "\n",
        "\"In the preceding code, we have allowed the environment to intialize its internal state.\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'In the preceding code, we have allowed the environment to intialize its internal state.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR0qR0qV5DtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba8f3b06-ef5d-4930-d923-cd30d9c13d44"
      },
      "source": [
        "def get_observation(self):\n",
        "  return [0.0, 0.0, 0.0]\n",
        "\n",
        "\" This get_observation() method is supposed to return the current environment's observation to the agent.\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\" This get_observation() method is supposed to return the current environment's observation to the agent.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjMiMtN5ZMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f9ba87f-6425-4eeb-fe66-8af1512cb862"
      },
      "source": [
        "def get_actions(self) -> bool:\n",
        "  return self.steps_left== 0\n",
        "\n",
        "\"This method signals to the agent the end of the episode. Keep in mind an episode can be finite (like chess) or infinite (Voyager missio game)\"\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'This method signals to the agent the end of the episode. Keep in mind an episode can be finite (like chess) or infinite (Voyager missio game)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjcYWom55-Tp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce24a660-fe68-4166-b6cf-4cb6d505754f"
      },
      "source": [
        "def action(self, action: int) -> float:\n",
        "  if self.is_done():\n",
        "    raise Exception('Game is over')\n",
        "  self.steps_left -= 1\n",
        "  return random.random()\n",
        "\n",
        "\"The action() method is the central piece in the environments functionality\"\n",
        "\"It does two things:\" \n",
        "\"- handles an agents actions\" \n",
        "\"- and returns the reward for this action\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'- and returns the reward for this action'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kuw42pH7Y4z",
        "colab_type": "text"
      },
      "source": [
        "In the constructor, we intialize the counter that will keep the total reward acummulated by the agent during the episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygUZnx2Y6yUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.total_reward= 0.0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4-aKO-58Gnd",
        "colab_type": "text"
      },
      "source": [
        "The step function accepts the environment instance as an argument and follows the agent to perform the following actions:\n",
        "  - observe the environment\n",
        "  - make a decision about the action to take based on the observations\n",
        "  -submit the action to the environment\n",
        "  - get the reward for the current step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-OVQYDG7YDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step(self, env: Environment):\n",
        "  current_obs= env.get_observation()\n",
        "  actions= env.get_actions()\n",
        "  reward= env.action(random.choice(actions))\n",
        "  self.total_reward += reward"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl2Qq2ze89oL",
        "colab_type": "text"
      },
      "source": [
        "The final piece is the glue code, which creates both classes and runs one episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4uWX4RY9Ga6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "dde7cbf4-3cde-4b36-abaf-9d390a8f4cbf"
      },
      "source": [
        "if __name__== \"__main__\":\n",
        "  env= Environment()\n",
        "  agent= Agent()\n",
        "  while not env.is_done():\n",
        "     agent.step(env)\n",
        "  print('Total reward got: %.4f' % agent.total_reward)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e3a1fa867e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m      \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total reward got: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'is_done'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDdJK1Wn0qgl",
        "colab_type": "text"
      },
      "source": [
        "The simplicity of the proceeding code illustrates the important basic concepts that come from the RL model.<br>\n",
        "The environment can be an extrememly complex one, and the agent can be a large neural network that implements the latest RL algorithm, BUT THE BASIC PATTERN WILL STAY THE SAME!!\n",
        "\n",
        "*On every step the agent will still take observations from an environment, do its calculations, and select the best action to take.<br>\n",
        "The results of this action will be some kind of reward (positive or negative) and a new observation where it repeats this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9taZZN2h6S",
        "colab_type": "text"
      },
      "source": [
        "#The OpenAI Gym API\n",
        "At a high level, every environment within OpenAI gym includes the following:\n",
        "  - A set of ACTIONS that is allowed to be executed within the environment.<br>\n",
        "      *Gym supports both discrete and continous actions, as well as their combinations\n",
        "  - the SHAPE and BOUNDARIES of the observations that the environment provides the agent with\n",
        "  - a method called STEP in order to execute an action, which returns the current observation, the reward, and the indication that the episode is over\n",
        "  - a method called RESET, which returns the environment to its initial state and obtains the first observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI_nVnuQ4Mq7",
        "colab_type": "text"
      },
      "source": [
        "#The Action Space\n",
        "The actions that an agent can execute can either be discrete, continous or even a combination of the two.<br>\n",
        "  - **discrete actions**: a fixed set of things that an agent can do. (For ex. directions in a grid such as left, right, up and down or a push button which can either be **pressed or released**.\n",
        "\n",
        "  - **continuos actions**: this type of action has a value attached to it.\n",
        "  Ex.) a steering wheel which can be turned at a specific angle, or an accelerator pedal, which can be pressed with different levels of force. \n",
        "\n",
        "A description of a continous action includes the boundaries of the valiue that the action could have.\n",
        "\n",
        "In the case of the steering wheel, it could be from -720 degrees to 720 degrees and for the accelerator pedal , it's usually from 0 to 1.\n",
        "\n",
        "*We are not limited to a single action....the environment could take multiple actions, such as pushing multiple buttons simultaneosly or steering the wheel and pressing two pedal (the brake and the accelerator).\n",
        "\n",
        "GYM defines a special container class that allows the nesting of several action spaces into one unified action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3keFNEiCkyk",
        "colab_type": "text"
      },
      "source": [
        "#The Observation Space\n",
        "Keep in mind that observations are pieces of information that an environment provides the agent with, on every timestamp, besides the reward.\n",
        "  *Observations can be as simple as a bunch of numbers or as complex as several multi-dimensional tensors containing color images from several cameras.<br>\n",
        "\n",
        "An observation can even be discrete, much like actions. (such as a lightbulb which can be in two states: on or off to use as a boolean value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1SEfzJRFLBk",
        "colab_type": "text"
      },
      "source": [
        "#The Environment\n",
        "The environment is represented in Gym by the Env class which has the following members:\n",
        "  - **action_space:** this is the field of the Space class and provides a specification for allowed actions in th environment\n",
        "  - **oservation_space:** this field has the same Space class, but specifies the observatons provided by the enviroment\n",
        "  - **reset():** this resets the environment to its initial state, returning the initial observation vector\n",
        "  - **step():** this method allows the agent to take action and returns information about the outcome of the action - the next observation, the local reward, and the end of episode flag.\n",
        "  (This method is a it complicated)\n",
        "\n",
        "**Theres also extra utility methods in Env class such as render(), which allows us to obtain the observation in a human- friendly form.\n",
        "\n",
        "Communications with the environment are performed via **step() and reset()**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssk5PAjGIPPW",
        "colab_type": "text"
      },
      "source": [
        "#Reset & Step\n",
        "The **reset** method is much simpler than the step method.<br>\n",
        "This method has no arguments and it instructs an environment to reset into it;s initial oservation.<br>\n",
        "  *Note that you have to call reset() after the creation of the enviroment because the agents communication with the environment may have an end (like Game Over) ***bold text***\n",
        "\n",
        "Such sessions are called episodes, and after the end of the episode, an agent needs to start over. (The value returned by this method id the first oservation of the environment)\n",
        "\n",
        "The step() method is the central piece within the environments functionality because it does several things in one call such as:\n",
        "  1. tells the environment which action we will execute on the next step\n",
        "  2. receives the new observation from the environment after this action\n",
        "  3. receives the reward the agent gained with this step\n",
        "  4. receives an indication that the episode is over\n",
        "\n",
        "*The first item (action) is passed as the only argument to this method, and the rest are returned by the step() method.<br>\n",
        "This will be presented as a tuple (not a Tuple class within Gym) with four elements (observation, reward, done, and info).\n",
        "  - observation: this is a numpy vector or a matrix with observation data\n",
        "  - reward: this is the float of the reward\n",
        "  - done: this is a Boolean indicator, which is True when the episode is over\n",
        "  - info: this could be anything the environment -specific with extra information about the environment\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w05IueuHL4EQ",
        "colab_type": "text"
      },
      "source": [
        "#Creating an Enviroment\n",
        "Every environment has a unique name of the **EnvironmentName-vN** form, where N is the number used to distinguish different versions of the same environment.<br>\n",
        "\n",
        "To create an environment, the **gym** package provides the **make(env_name)** function, whose only argument is the environment's name in string form.<br>\n",
        "\n",
        "Gym offers several groups within it's RL platform that you can play with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vldEBTlMzMSP",
        "colab_type": "text"
      },
      "source": [
        "#Extra Gym Functionality - Wrappers and Monitors\n",
        "Wrappers\n",
        "Very frequently we may want to extend the environments functionality in some generic way. <br>\n",
        "For ex) Imagine an environment gives you some type of observation, but you want to accumulate them in some buffer and provide to the agent the N last observations. (This is a common scenario for dynamic computer games, when on single frame is just not enough to get the full inforation about the game state.<br>\n",
        "There are many such situations that have the same structure you want to wrap the existing environment and add some extra logic for doing something. Gym provides a convenient framework for these situations - the Wrapper Class.\n",
        "\n",
        "*Full example of the code is within the Github repo under:<br>\n",
        "***Chapter02/03_random_action_wrapper.py***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1TKSZiCUs9",
        "colab_type": "text"
      },
      "source": [
        "Below we initialized our wrapper (random) by calling the parents *__init__ *method and then saving epsilon (the proability of the random action)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ll7yJ60AL11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from typing import TypeVar\n",
        "import random\n",
        "\n",
        "Action= TypeVar('Action')\n",
        "class RandomActionWrapper(gym.ActionWrapper):\n",
        "  def __init__(self, env, epsilon= 0.1):\n",
        "    super(RandomActionWrapper, self).__init__(env)\n",
        "    self.epsilon= epsilon"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZdBRwPuB8dd",
        "colab_type": "text"
      },
      "source": [
        "A method used to override from a parents class to tweak the agents actions. <br>\n",
        "Everytime we roll the die, and with the probability of epsilon, we sample a random action from the action space and return it instead of the action the agent has sent us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGrRotmABqmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def action(self, action:Action) -> Action:\n",
        "  if random.random() < self.epsilon:\n",
        "    print('Random!')\n",
        "    return self.env.action_space.sample()\n",
        "  return action"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnYowktrCxhE",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to apply our wrapper. <br>\n",
        "We will create a normal CartPole environment and pass it to our **Wrapper** constuctor.<br>\n",
        "From here on, we will use the wrapper as a normal **Env** instance instead of the original CartPole. <br>\n",
        "*As the Wrapper class inherits the Env class and exposes the same interface, we can nest our wrappers in any combination we want.<br>\n",
        "*THIS IS VERY POWERFUL ELEGANT, AND GENERIC SOLUTION!!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIqoqSoeDcyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__== '__main__':\n",
        "  env= RandomActionWrapper(gym.make('CartPole-v0'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0WkOPmEDoJD",
        "colab_type": "text"
      },
      "source": [
        "Here is almost the same code, except that everytime we issue the same action, **0**, our agent is dull and does the same thing.<br>\n",
        "By running the code, we should see that the wrapper is indeed working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oom3OeTeD6uh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1982725a-ee97-4cf5-dc7a-46965a1edcc8"
      },
      "source": [
        "obs= env.reset()\n",
        "total_reward= 0.0\n",
        "while True:\n",
        "  obs, reward, done, _= env.step(0)\n",
        "  total_reward += reward\n",
        "  if done:\n",
        "    break\n",
        "print('Reward got: %.2f' % total_reward)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7afc8fdfc86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreverse_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4NFXcznEN8F",
        "colab_type": "text"
      },
      "source": [
        "If we want, we can play with the epsilon parameter on the wrapper's creation and verify that randomness imprlves the agents score *on average.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrqT9yYmEdrY",
        "colab_type": "text"
      },
      "source": [
        "#Monitor\n",
        "This type of class is implemented like Wrapper and can write information about your agents performance ina file, ***with an optional video recording of your agent in action.***<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EgLkNEvFHF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__== '__main__':\n",
        "  env= gym.make('CartPole-v0')\n",
        "  env= gym.wrappers.Monitor(env, 'recording')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC9IMfLfFZkP",
        "colab_type": "text"
      },
      "source": [
        "The Monitor class requires that you have **FFmpeg** utlity present on your local system.<br>\n",
        "This enables you to capture observations into an output video file.<br>\n",
        "*This utility must be available, other Monitor will raise an exception.\n",
        "**bold text**\n",
        "Below is the simplest way to install FFmpeg to your local system using one of these three approaches:\n",
        "  - the code should be run in an X11 session with the OpenGL extension(GLX)\n",
        "  - the code should be started in an Xvfb virtual display\n",
        "  - you can use X11 forwarding in an SSh connection\n",
        "\n",
        "*The reason for this is video recording, which is done by taking screenshots of the window drawn by the environment. ***bold text***\n",
        "\n",
        "In order to accomplish this in the cloud instead of using our local machine, all we have to do is use a special 'virtual' graphical display, called **Vxfb(X11 virtual framebuffer)**, which basically starts a virtual graphical display on the server and forces the program to draw inside of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZyaiDRhKP_K",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "In this notebook we learned the following:\n",
        "  - installed the OpenAI gym\n",
        "  - studied it's basic API and created a randomly behaving agent\n",
        "  - learned how to extend the functionality of existing environments in a modular way\n",
        "  - became familiar with a way to record our agents activity using the **Monitor **class (heavily used)"
      ]
    }
  ]
}